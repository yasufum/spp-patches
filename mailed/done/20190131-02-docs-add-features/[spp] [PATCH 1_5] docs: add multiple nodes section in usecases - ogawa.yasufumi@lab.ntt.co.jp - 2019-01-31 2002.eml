X-Account-Key: account1
X-UIDL: 13845.Mf4RsBcG9hMpQqaO0MVpOVjNh6w=
X-Mozilla-Status: 0001
X-Mozilla-Status2: 00000000
X-Mozilla-Keys:                                                                                 
Return-Path: spp-bounces@dpdk.org
Received: from md-lpx-cb002.ocn.ad.jp (LHLO md-lpx-cb002) (153.138.210.15)
 by mzcstore251.ocn.ad.jp with LMTP; Thu, 31 Jan 2019 20:04:30 +0900 (JST)
Received: from md-mta-nw006.ocn.ad.jp ([153.138.217.133])
	by md-lpx-cb002 with LMTP id mE+GMj7WUlyISAAAtwVFfA
	; Thu, 31 Jan 2019 20:04:30 +0900
Received: from mfgw691.ocn.ad.jp (mfgw691.ocn.ad.jp [153.153.63.98])
	by md-mta-nw006.ocn.ad.jp (Postfix) with ESMTP id B312E3C000350
	for <geminoa@juno.ocn.ne.jp>; Thu, 31 Jan 2019 20:04:30 +0900 (JST)
Received-SPF: pass (mf-ofc-ucb069: domain designate client-ip as permitted sender) client-ip=92.243.14.124; envelope-from=<spp-bounces@dpdk.org>; helo=dpdk.org;
Authentication-Results: mf-ofc-ucb069; spf=pass smtp.mailfrom=spp-bounces@dpdk.org
Received: from dpdk.org (dpdk.org [92.243.14.124])
	by mfgw691.ocn.ad.jp (Postfix) with ESMTP id C6E1AA8027A
	for <geminoa@juno.ocn.ne.jp>; Thu, 31 Jan 2019 20:04:29 +0900 (JST)
Received: from [92.243.14.124] (localhost [127.0.0.1])
	by dpdk.org (Postfix) with ESMTP id EE5431B453;
	Thu, 31 Jan 2019 12:04:25 +0100 (CET)
Received: from tama500.ecl.ntt.co.jp (tama500.ecl.ntt.co.jp [129.60.39.148])
 by dpdk.org (Postfix) with ESMTP id BF6E01B43D
 for <spp@dpdk.org>; Thu, 31 Jan 2019 12:04:23 +0100 (CET)
Received: from vc1.ecl.ntt.co.jp (vc1.ecl.ntt.co.jp [129.60.86.153])
 by tama500.ecl.ntt.co.jp (8.13.8/8.13.8) with ESMTP id x0VB4MK5030110;
 Thu, 31 Jan 2019 20:04:22 +0900
Received: from vc1.ecl.ntt.co.jp (localhost [127.0.0.1])
 by vc1.ecl.ntt.co.jp (Postfix) with ESMTP id 8B09FEA8506;
 Thu, 31 Jan 2019 20:04:22 +0900 (JST)
Received: from localhost.localdomain (lobster.nslab.ecl.ntt.co.jp
 [129.60.13.95])
 by vc1.ecl.ntt.co.jp (Postfix) with ESMTP id 7ABE7EA8475;
 Thu, 31 Jan 2019 20:04:22 +0900 (JST)
From: ogawa.yasufumi@lab.ntt.co.jp
To: ferruh.yigit@intel.com, spp@dpdk.org, ogawa.yasufumi@lab.ntt.co.jp
Date: Thu, 31 Jan 2019 20:02:10 +0900
Message-Id: <1548932534-20983-2-git-send-email-ogawa.yasufumi@lab.ntt.co.jp>
X-Mailer: git-send-email 2.7.4
In-Reply-To: <1548932534-20983-1-git-send-email-ogawa.yasufumi@lab.ntt.co.jp>
References: <1548932534-20983-1-git-send-email-ogawa.yasufumi@lab.ntt.co.jp>
X-TM-AS-MML: disable
Subject: [spp] [PATCH 1/5] docs: add multiple nodes section in usecases
X-BeenThere: spp@dpdk.org
X-Mailman-Version: 2.1.15
Precedence: list
List-Id: Soft Patch Panel <spp.dpdk.org>
List-Unsubscribe: <https://mails.dpdk.org/options/spp>,
 <mailto:spp-request@dpdk.org?subject=unsubscribe>
List-Archive: <http://mails.dpdk.org/archives/spp/>
List-Post: <mailto:spp@dpdk.org>
List-Help: <mailto:spp-request@dpdk.org?subject=help>
List-Subscribe: <https://mails.dpdk.org/listinfo/spp>,
 <mailto:spp-request@dpdk.org?subject=subscribe>
Errors-To: spp-bounces@dpdk.org
Sender: "spp" <spp-bounces@dpdk.org>

From: Yasufumi Ogawa <ogawa.yasufumi@lab.ntt.co.jp>

In `Use Case`, usecase of multiple nodes is described in `spp_nfv`
section, but it should be moved as a dependent one. This update is to
add `Multiple Nodes` in `Use Case` and move contents described in
`spp_nfv` to the section.

Signed-off-by: Yasufumi Ogawa <ogawa.yasufumi@lab.ntt.co.jp>
---
 docs/guides/use_cases/index.rst       |   1 +
 docs/guides/use_cases/multi_nodes.rst | 174 ++++++++++++++++++++++++++++++++++
 docs/guides/use_cases/spp_nfv.rst     | 170 ---------------------------------
 3 files changed, 175 insertions(+), 170 deletions(-)
 create mode 100644 docs/guides/use_cases/multi_nodes.rst

diff --git a/docs/guides/use_cases/index.rst b/docs/guides/use_cases/index.rst
index c41dc2e..22c946b 100644
--- a/docs/guides/use_cases/index.rst
+++ b/docs/guides/use_cases/index.rst
@@ -9,3 +9,4 @@ Use Cases
    :numbered:
 
    spp_nfv
+   multi_nodes
diff --git a/docs/guides/use_cases/multi_nodes.rst b/docs/guides/use_cases/multi_nodes.rst
new file mode 100644
index 0000000..6be567e
--- /dev/null
+++ b/docs/guides/use_cases/multi_nodes.rst
@@ -0,0 +1,174 @@
+..  SPDX-License-Identifier: BSD-3-Clause
+    Copyright(c) 2019 Nippon Telegraph and Telephone Corporation
+
+
+.. _usecase_multi_node:
+
+Multiple Nodes
+==============
+
+SPP provides multi-node support for configuring network across several nodes
+from SPP CLI. You can configure each of nodes step by step.
+
+In :numref:`figure_spp_multi_nodes_vhost`, there are four nodes on which
+SPP and service VMs are running. Host1 behaves as a patch panel for connecting
+between other nodes. A request is sent from a VM on host2 to a VM on host3 or
+host4. Host4 is a backup server for host3 and replaced with host3 by changing
+network configuration. Blue lines are paths for host3 and red lines are for
+host4, and changed alternatively.
+
+.. _figure_spp_multi_nodes_vhost:
+
+.. figure:: ../images/setup/use_cases/spp_multi_nodes_vhost.*
+   :width: 100%
+
+   Multiple nodes example
+
+Launch SPP on Multiple Nodes
+----------------------------
+
+Before SPP CLI, launch spp-ctl on each of nodes. You should give IP address
+with ``-b`` option to be accessed from outside of the node.
+This is an example for launching spp-ctl on host1.
+
+.. code-block:: console
+
+    # Launch on host1
+    $ python3 src/spp-ctl/spp-ctl -b 192.168.11.101
+
+You also need to launch it on host2, host3 and host4 in each of terminals.
+
+After all of spp-ctls are lauched, launch SPP CLI with four ``-b`` options
+for each of hosts. SPP CLI is able to be launched on any of nodes.
+
+.. code-block:: console
+
+    # Launch SPP CLI
+    $ python src/spp.py -b 192.168.11.101 \
+        -b 192.168.11.102 \
+        -b 192.168.11.103 \
+        -b 192.168.11.104 \
+
+If you succeeded to launch all of processes before, you can find them
+by running ``sever list`` command.
+
+.. code-block:: console
+
+    # Launch SPP CLI
+    spp > server list
+      1: 192.168.1.101:7777 *
+      2: 192.168.1.102:7777
+      3: 192.168.1.103:7777
+      4: 192.168.1.104:7777
+
+You might notice that first entry is marked with ``*``. It means that
+the current node under the management is the first node.
+
+Switch Nodes
+------------
+
+SPP CLI manages a node marked with ``*``. If you configure other nodes,
+change the managed node with ``server`` command.
+Here is an example to switch to third node.
+
+.. code-block:: console
+
+    # Launch SPP CLI
+    spp > server 3
+    Switch spp-ctl to "3: 192.168.1.103:7777".
+
+And the result after changed to host3.
+
+.. code-block:: console
+
+    spp > server list
+      1: 192.168.1.101:7777
+      2: 192.168.1.102:7777
+      3: 192.168.1.103:7777 *
+      4: 192.168.1.104:7777
+
+You can also confirm this change by checking IP address of spp-ctl from
+``status`` command.
+
+.. code-block:: console
+
+    spp > status
+    - spp-ctl:
+      - address: 192.168.1.103:7777
+    - primary:
+      - status: not running
+    ...
+
+Configure Patch Panel Node
+--------------------------
+
+First of all of the network configuration, setup blue lines on host1
+described in :numref:`figure_spp_multi_nodes_vhost`.
+You should confirm the managed server is host1.
+
+.. code-block:: console
+
+    spp > server list
+      1: 192.168.1.101:7777 *
+      2: 192.168.1.102:7777
+      ...
+
+Patch two sets of physical ports and start forwarding.
+
+.. code-block:: console
+
+    spp > nfv 1; patch phy:1 phy:2
+    Patch ports (phy:1 -> phy:2).
+    spp > nfv 1; patch phy:3 phy:0
+    Patch ports (phy:3 -> phy:0).
+    spp > nfv 1; forward
+    Start forwarding.
+
+Configure Service VM Nodes
+--------------------------
+
+It is almost similar as
+:ref:`Setup Network Configuration in spp_nfv<usecase_spp_nfv_l2fwd_vhost_nw>`
+to setup for host2, host3, and host4.
+
+For host2, swith server to host2 and run nfv commands.
+
+.. code-block:: console
+
+    # switch to server 2
+    spp > server 2
+    Switch spp-ctl to "2: 192.168.1.102:7777".
+
+    # configure
+    spp > nfv 1; patch phy:0 vhost:0
+    Patch ports (phy:0 -> vhost:0).
+    spp > nfv 1; patch vhost:0 phy:1
+    Patch ports (vhost:0 -> phy:1).
+    spp > nfv 1; forward
+    Start forwarding.
+
+Then, swith to host3 and host4 for doing the same configuration.
+
+Change Path to Backup Node
+--------------------------
+
+Finally, change path from blue lines to red lines.
+
+.. code-block:: console
+
+    # switch to server 1
+    spp > server 2
+    Switch spp-ctl to "2: 192.168.1.102:7777".
+
+    # remove blue path
+    spp > nfv 1; stop
+    Stop forwarding.
+    spp > nfv 1; patch reset
+
+    # configure red path
+    spp > nfv 2; patch phy:1 phy:4
+    Patch ports (phy:1 -> phy:4).
+    spp > nfv 2; patch phy:5 phy:0
+    Patch ports (phy:5 -> phy:0).
+    spp > nfv 2; forward
+    Start forwarding.
diff --git a/docs/guides/use_cases/spp_nfv.rst b/docs/guides/use_cases/spp_nfv.rst
index 8a8e509..39204e3 100644
--- a/docs/guides/use_cases/spp_nfv.rst
+++ b/docs/guides/use_cases/spp_nfv.rst
@@ -497,173 +497,3 @@ and watch received packets on pktgen.
     Start forwarding.
 
 After started forwarding, you can see that packet count is increased.
-
-
-Multiple Nodes
---------------
-
-SPP provides multi-node support for configuring network across several nodes
-from SPP CLI. You can configure each of nodes step by step.
-
-In :numref:`figure_spp_multi_nodes_vhost`, there are four nodes on which
-SPP and service VMs are running. Host1 behaves as a patch panel for connecting
-between other nodes. A request is sent from a VM on host2 to a VM on host3 or
-host4. Host4 is a backup server for host3 and replaced with host3 by changing
-network configuration. Blue lines are paths for host3 and red lines are for
-host4, and changed alternatively.
-
-.. _figure_spp_multi_nodes_vhost:
-
-.. figure:: ../images/setup/use_cases/spp_multi_nodes_vhost.*
-   :width: 100%
-
-   Multiple nodes example
-
-Launch SPP on Multiple Nodes
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Before SPP CLI, launch spp-ctl on each of nodes. You should give IP address
-with ``-b`` option to be accessed from outside of the node.
-This is an example for launching spp-ctl on host1.
-
-.. code-block:: console
-
-    # Launch on host1
-    $ python3 src/spp-ctl/spp-ctl -b 192.168.11.101
-
-You also need to launch it on host2, host3 and host4 in each of terminals.
-
-After all of spp-ctls are lauched, launch SPP CLI with four ``-b`` options
-for each of hosts. SPP CLI is able to be launched on any of nodes.
-
-.. code-block:: console
-
-    # Launch SPP CLI
-    $ python src/spp.py -b 192.168.11.101 \
-        -b 192.168.11.102 \
-        -b 192.168.11.103 \
-        -b 192.168.11.104 \
-
-If you succeeded to launch all of processes before, you can find them
-by running ``sever list`` command.
-
-.. code-block:: console
-
-    # Launch SPP CLI
-    spp > server list
-      1: 192.168.1.101:7777 *
-      2: 192.168.1.102:7777
-      3: 192.168.1.103:7777
-      4: 192.168.1.104:7777
-
-You might notice that first entry is marked with ``*``. It means that
-the current node under the management is the first node.
-
-Switch Nodes
-~~~~~~~~~~~~
-
-SPP CLI manages a node marked with ``*``. If you configure other nodes,
-change the managed node with ``server`` command.
-Here is an example to switch to third node.
-
-.. code-block:: console
-
-    # Launch SPP CLI
-    spp > server 3
-    Switch spp-ctl to "3: 192.168.1.103:7777".
-
-And the result after changed to host3.
-
-.. code-block:: console
-
-    spp > server list
-      1: 192.168.1.101:7777
-      2: 192.168.1.102:7777
-      3: 192.168.1.103:7777 *
-      4: 192.168.1.104:7777
-
-You can also confirm this change by checking IP address of spp-ctl from
-``status`` command.
-
-.. code-block:: console
-
-    spp > status
-    - spp-ctl:
-      - address: 192.168.1.103:7777
-    - primary:
-      - status: not running
-    ...
-
-Configure Patch Panel Node
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-First of all of the network configuration, setup blue lines on host1
-described in :numref:`figure_spp_multi_nodes_vhost`.
-You should confirm the managed server is host1.
-
-.. code-block:: console
-
-    spp > server list
-      1: 192.168.1.101:7777 *
-      2: 192.168.1.102:7777
-      ...
-
-Patch two sets of physical ports and start forwarding.
-
-.. code-block:: console
-
-    spp > nfv 1; patch phy:1 phy:2
-    Patch ports (phy:1 -> phy:2).
-    spp > nfv 1; patch phy:3 phy:0
-    Patch ports (phy:3 -> phy:0).
-    spp > nfv 1; forward
-    Start forwarding.
-
-Configure Service VM Nodes
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-It is almost similar as
-:ref:`Setup Network Configuration in spp_nfv<usecase_spp_nfv_l2fwd_vhost_nw>`
-to setup for host2, host3, and host4.
-
-For host2, swith server to host2 and run nfv commands.
-
-.. code-block:: console
-
-    # switch to server 2
-    spp > server 2
-    Switch spp-ctl to "2: 192.168.1.102:7777".
-
-    # configure
-    spp > nfv 1; patch phy:0 vhost:0
-    Patch ports (phy:0 -> vhost:0).
-    spp > nfv 1; patch vhost:0 phy:1
-    Patch ports (vhost:0 -> phy:1).
-    spp > nfv 1; forward
-    Start forwarding.
-
-Then, swith to host3 and host4 for doing the same configuration.
-
-Change Path to Backup Node
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Finally, change path from blue lines to red lines.
-
-.. code-block:: console
-
-    # switch to server 1
-    spp > server 2
-    Switch spp-ctl to "2: 192.168.1.102:7777".
-
-    # remove blue path
-    spp > nfv 1; stop
-    Stop forwarding.
-    spp > nfv 1; patch reset
-
-    # configure red path
-    spp > nfv 2; patch phy:1 phy:4
-    Patch ports (phy:1 -> phy:4).
-    spp > nfv 2; patch phy:5 phy:0
-    Patch ports (phy:5 -> phy:0).
-    spp > nfv 2; forward
-    Start forwarding.
-- 
2.7.4


